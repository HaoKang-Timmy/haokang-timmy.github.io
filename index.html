<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hao Kang</title>
  
  <meta name="author" content="Hao Kang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/mit_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hao Kang(康浩)</name>
              </p>
              <p>I am a PhD student at Georgia Institute of Technology advised by Prof. <a href="https://ece.gatech.edu/directory/tushar-krishna">Tushar Krishna</a> Currently, I am visiting MIT as a visiting student advised by Prof. <a href="https://songhan.mit.edu/">Song Han</a> and work closely with <a href="https://han-cai.github.io/"> Han Cai</a>.
              </p>
              <p>
                Prior to GT,  I was fortunate to have worked with Prof <a href="https://scholar.google.com/citations?user=x63j7HEAAAAJ&hl=en">Baharan</a> at UCLA about <a href="http://web.cs.ucla.edu/~baharan/research.htm">efficient machine learning from massive datasets</a>. 
                At MIT, I work with Prof <a href="https://songhan.mit.edu/">Song Han</a> about efficient machine learning on edge device.  I received my B.Eng. in Computer Science in 2023 from Zhejiang University.
              </p>
              <p style="text-align:center">
                <a href="mailto:hao.kang.sys@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/kh_cv10.15.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?hl=en&user=8NHgb2AAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/XinjingC">Twitter</a> &nbsp/&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/xinjing-zhou-0a1a40a3/">Linkedin</a> &nbsp/&nbsp -->
                <a href="https://github.com/HaoKang-Timmy">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/43510722.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;padding-top:5px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
                I am working in machine learning systems. In the first two years(2023-2025), I focused on efficient quantization and sparsity of LLMs to reduce serving costs. I learnt how to wrote cuda kernels through these projects. Now I am working on Agentic LLM Efficiency and system-algorithm co-design for LLM architectures.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;padding-top:5px;">
              <heading>SelectedPublished Papers</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
              <a href="https://www.competitive-agent.com">
                <papertitle>Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs
                </papertitle>
              </a>
              <br>
    
              Neurips 2025 Spotlight
              <br>
              <br>
    
              Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman
              <br>
              TurboAttention integrates two core innovations—FlashQ for headwise KV-cache and activation quantization, and SAS for dequantization-free softmax—achieving 1.2–1.8× faster attention, >4.4× KV-cache reduction, and up to 2.37× higher throughput than FP16 while surpassing prior compression and quantization methods.
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.08585">
                <papertitle>TurboAttention: Efficient Attention Approximation For High Throughputs LLMs
                </papertitle>
              </a>
              <br>
    
              Mlsys 2025
              <br>
              <br>
    
              Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan
              <br>
              TurboAttention integrates two core innovations—FlashQ for headwise KV-cache and activation quantization, and SAS for dequantization-free softmax—achieving 1.2–1.8× faster attention, >4.4× KV-cache reduction, and up to 2.37× higher throughput than FP16 while surpassing prior compression and quantization methods.
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.05527">
                <papertitle>GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
                </papertitle>
              </a>
              <br>
    
              Neurips 2024 ENLSP Best Paper Candidate
              <br>
              <br>
    
              Hao Kang*, Qingru Zhang*, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao
              <br>
              We propose a novel cache compression algorithm on KV cache for large language model inference. It can achieve near-lossless compression ratio and 2x speedup and 2x peak memory saving on inference time.
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/pdf/2306.01244.pdf"> -->
                <papertitle>KV Cache Optimizations for Large Language Model Inference
                </papertitle>
              </a>
              <p>In review Mlsys2024</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.01244.pdf">
                <papertitle>Towards Sustainable Learning: Coresets for Data-efficient Deep Learning
                </papertitle>
              </a>
              <br>
    
              ICML2023
              <br>
              <br>
    
              Yu Yang, Hao Kang, Baharan Mirzasoleiman
              <br>
              We design a dataset distiling algorithm based on submodular function and batch SGD that can distil a small dataset from a large dataset. The small dataset can be used to train a model with similar performance to the model trained on the large dataset.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;padding-top:5px;">
              <heading>Research Project and Tools</heading>
            </td>
          </tr>


          <tr>
            <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
              <a href="https://github.com/HaoKang-Timmy/torchanalyse.git">
                <papertitle>torchanalyse.
                </papertitle>
              </a>
              <br>
    
              A model profiling tool based on TVM and Maestro(Thanks for the help Abhi!). It can profile the model and give the flops, memory usage, and latency of each layer. It can also give the flops of each operator in the model.
              <br>
              <a href="data/Epipe.key">More Information</a>
              <p></p>
            </td>
          </tr>
		
       <tr>
        <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
          <a href="https://github.com/HaoKang-Timmy/Epipe">
            <papertitle>Epipe: Efficient Pipeline Parallelism with Compression Algorithms.
            </papertitle>
          </a>
          <br>

          A research project based on <a href="https://github.com/kakaobrain/torchgpipe">Gpipe</a> and low-rank approximation which decreases bandwidth of activation transfer during cloud-based training process.
          <br>
          <a href="data/Epipe.key">More Information</a>
          <p></p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px; padding-top:5px; width:75%;vertical-align:middle">
          <a href="https://github.com/Lyken17/pytorch-OpCounter">
            <papertitle>THOP: PyTorch-OpCounter.
            </papertitle>
          </a>
          <br>

          A python third party library that counts flops of models(pytorch, jit, onnx). Already has 4k stars! I wrote the counter of onnx form.
          <br>
          <a href="https://github.com/Lyken17/pytorch-OpCounter/pull/143">More Information</a>
          <p></p>
        </td>
      </tr>


    
        <!-- </tbody></table>
      
        <table width="100%" align="center" border="0" cellpadding="5"><tbody>
          <tr><td style="padding:20px;padding-top:5px;width:100%;vertical-align:middle">
            <heading>Industry Experiences</heading>
          </td>
      </tr></tbody></table>
				
      <table width="100%" align="center" border="0" cellpadding="5"><tbody>
        <tr><td style="padding:20px;padding-top:5px;width:100%;vertical-align:middle">
          <a href="https://www.dolphindb.com/"><papertitle>DolphinDB, Inc.</papertitle></a>
          <p>Senior Software Engineer    |    03/2021 - 09/2021, Hangzhou, China</p>
          Designing and implementing a new time-series storage engine of DolphinDB for IoT and financial applications.
        </td>
    </tr></tbody></table>

      <table width="100%" align="center" border="0" cellpadding="5"><tbody>
        <tr><td style="padding:20px;padding-top:5px;width:100%;vertical-align:middle">
          <a href="https://www.tencent.com/"><papertitle>Tencent</papertitle></a>
          <p>Software Engineer   |    03/2020 - 03/2021, Shanghai, China</p>
          Working on kernels of CynosDB/CDB (MySQL fork).
        </td>
    </tr></tbody></table>

    <table width="100%" align="center" border="0" cellpadding="5"><tbody>
      <tr><td style="padding:20px;padding-top:5px;width:100%">
        <heading>Internships</heading>
      </td>
  </tr></tbody></table>

    <table width="100%" align="center" border="0" cellpadding="5"><tbody>
      <tr><td style="padding:20px;padding-top:5px;width:100%;vertical-align:middle">
        <a href="https://www.alibaba.com/"><papertitle>Alibaba</papertitle></a>
        <p>Software Engineer Intern  |    08/2019 - 09/2019, Hangzhou, China</p>
        Working on the DPTree project.
      </td>
  </tr></tbody></table>

  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr><td style="padding:20px;padding-top:5px;width:100%;vertical-align:middle">
      <a href="https://www.dolphindb.com/"><papertitle>DolphinDB, Inc.</papertitle></a>
      <p>Software Engineer Intern  |    05/2017 - 01/2019, Hangzhou, China</p>
    </td>
</tr></tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
              <p>
                <li>
                  External reviewer for SIGMOD 2021, VLDB 2021
                </li>
                <li>
                  External reviewer for VLDBJ 2022.
                </li>
               </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br><br>
              <p style="text-align:right;font-size:small;">
                Template from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron.</a>
              </p>
          </td></tr>
        </tbody></table>
      </td>
    </tr>
  </table> -->

  
</body>

</html>
